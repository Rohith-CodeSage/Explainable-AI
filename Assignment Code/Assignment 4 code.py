# -*- coding: utf-8 -*-
"""explainable_ASS_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rlh0_RS5JTNS1Diw_oDH5AZ_6RCilMcv
"""

# Titanic Survival Prediction with Explainable AI (Logistic Regression)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.inspection import permutation_importance

import shap
import lime
import lime.lime_tabular

# -------------------------------------------------------
# 1. Load Titanic dataset
# -------------------------------------------------------
url = "https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv"
data = pd.read_csv(url)

# Basic preprocessing
# Fill missing values
data["Age"].fillna(data["Age"].median(), inplace=True)
data["Embarked"].fillna(data["Embarked"].mode()[0], inplace=True)

# Select features
features = ["Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Embarked"]
X = data[features]
y = data["Survived"]

# Encode categorical variables
encoders = {}
for col in ["Sex", "Embarked"]:
    encoders[col] = LabelEncoder()
    X[col] = encoders[col].fit_transform(X[col])

# -------------------------------------------------------
# 2. Train-test split
# -------------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# -------------------------------------------------------
# 3. Logistic Regression Model
# -------------------------------------------------------
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

# -------------------------------------------------------
# 4. Permutation Importance
# -------------------------------------------------------
perm_importance = permutation_importance(model, X_test, y_test, n_repeats=20, random_state=42)
importance_df = pd.DataFrame({
    "Feature": X.columns,
    "Importance": perm_importance.importances_mean
}).sort_values(by="Importance", ascending=False)

print("\nPermutation Importance:\n", importance_df)

# Plot
plt.figure(figsize=(7,4))
sns.barplot(x="Importance", y="Feature", data=importance_df)
plt.title("Permutation Importance")
plt.show()

# -------------------------------------------------------
# 5. SHAP Explanations (Global + Local)
# -------------------------------------------------------
explainer = shap.LinearExplainer(model, X_train, feature_names=X.columns)
shap_values = explainer.shap_values(X_test)

# Global importance
shap.summary_plot(shap_values, X_test, feature_names=X.columns)

# Local explanation for first passenger
shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0], matplotlib=True)

# -------------------------------------------------------
# 6. LIME Explanations
# -------------------------------------------------------
lime_explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_train),
    feature_names=X.columns,
    class_names=["Died", "Survived"],
    mode="classification"
)

# Explain first passenger
idx = 0
exp = lime_explainer.explain_instance(
    data_row=X_test.iloc[idx],
    predict_fn=model.predict_proba
)
exp.show_in_notebook(show_table=True)

# Second passenger
idx = 1
exp2 = lime_explainer.explain_instance(
    data_row=X_test.iloc[idx],
    predict_fn=model.predict_proba
)
exp2.show_in_notebook(show_table=True)

# -------------------------------------------------------
# 7. Comparison
# -------------------------------------------------------
print("\nComparison of XAI Methods:")
print("- Permutation Importance: Ranks global feature importance.")
print("- SHAP: Provides both global and local explanations, showing impact of each feature.")
print("- LIME: Gives case-specific local explanations, highlighting how features affect one prediction.")